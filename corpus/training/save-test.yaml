# Minimal test to verify model weights are saved
# Uses 1 epoch with small sequence length

model:
  path: "/home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a"
  mode: transformer
  config: "/home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/config.json"

data:
  train: "/home/noah/src/HF-Advanced-Fine-Tuning/corpus/data/corpus/train_small.jsonl"
  tokenizer: "/home/noah/.cache/huggingface/hub/models--Qwen--Qwen2.5-Coder-1.5B-Instruct/snapshots/2e1fd397ee46e1388853d2af2c993145b0f1098a/tokenizer.json"
  batch_size: 1
  seq_len: 32
  input_column: text

optimizer:
  name: "adamw"
  lr: 0.0001
  weight_decay: 0.01

training:
  epochs: 1
  mode: causal_lm
  gradient_accumulation: 1
  warmup_steps: 0
  output_dir: "/home/noah/src/HF-Advanced-Fine-Tuning/corpus/checkpoints/save-test"
  seed: 42
